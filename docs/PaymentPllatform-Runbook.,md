Payments Platform — Kafka Consumer Runbook

Last updated: {{today}}

0) TL;DR
	•	Throughput lever: partitions × total listener concurrency.
	•	Capacity rule: required_parallelism ≈ arrival_rate × commit_p95 × safety_factor(1.2–1.5).
	•	Retry math: expected retry traffic = created_rate × r/(1−r) (e.g., r=0.20 ⇒ +25%).
	•	Baseline (NORMAL PSP): 8 partitions for hot topics; 2 pods × 4 listeners each; per-record transactional processing; lag ≈ flat at 10 rps.

⸻

1) Scope & Architecture

Apps
	•	payment-service — writes DB + Outbox; dispatches events to Kafka.
	•	payment-consumers — consumes PaymentOrderCreated and PaymentOrderRetryRequested, calls PSP, writes state, produces follow‑ups, commits offsets transactionally.

Topics
	•	payment_order_created_topic — 8 partitions
	•	payment_order_retry_request_topic — 8 partitions
	•	payment_order_succeeded_topic — 8 partitions
	•	payment_status_check_scheduler_topic — 1 partition
	•	Each has a matching .DLQ with the same partition count as the source.

Ordering
	•	Key by paymentOrderId so all events for an order land on the same partition.

⸻

2) Golden Signals & SLOs

Golden signals
	•	Consumer lag per group & per partition
	•	Queueing delay (now − record.timestamp())
	•	PSP latency (network + simulator)
	•	Commit time per record (listener start → tx commit including DB)
	•	Fetch latency (Kafka consumer metric)

Targets (NORMAL)
	•	Commit p95 ≤ 250 ms
	•	Queueing p95 ≤ 200–300 ms at 10–20 ev/s
	•	Lag spikes recover within < 60 s; slope returns to ~0.

Alerting
	•	Warn: group lag > 1,000 for 5m; Page: > 10,000 for 5m
	•	Alert on max.poll.interval exceeded or rebalance storms

⸻

3) Sizing Formulas
	•	Required parallelism: arrival_rate (ev/s) × commit_p95 (s) × 1.2–1.5
	•	Expected retries per order: r/(1−r) ⇒ traffic to retry topic ≈ created_rate × r/(1−r)
	•	Example (NORMAL): r=0.20 → +25% retry load; 1/(1−r)=1.25 attempts/order

⸻

4) Baseline Config (working)

Kafka topics (payment‑service)

app:
  kafka:
    specs:
      payment_order_created_topic:        { partitions: 8, replicas: 1, createDlq: true }
      payment_order_retry_request_topic:  { partitions: 8, replicas: 1, createDlq: true }
      payment_order_succeeded_topic:      { partitions: 8, replicas: 1, createDlq: true }
      payment_status_check_scheduler_topic: { partitions: 1, replicas: 1, createDlq: true }

Consumers (payment‑consumers)
	•	Container factory: ackMode=MANUAL, isBatchListener=false, concurrency from config (no concurrency="1" on @KafkaListener).
	•	Consumer props

spring:
  kafka:
    consumer:
      enable-auto-commit: false
      isolation.level: read_committed
      max-poll-records: 25
      properties:
        fetch.max.wait.ms: 50
        partition.assignment.strategy: org.apache.kafka.clients.consumer.CooperativeStickyAssignor
    producer:
      properties:
        enable.idempotence: true
        linger.ms: 5
        compression.type: lz4

Deployment
	•	Partitions: 8 (created/retry/succeeded)
	•	Consumers: 2 pods × concurrency=4 (total = 8 listeners) for created & retry
	•	Status check executor: concurrency 1 (topic has 1 partition)
	•	PSP pools per pod: ~4 threads, queue capacity 0–16, CallerRunsPolicy
	•	Unique transaction-id-prefix per pod (EOS)

HPA
	•	For now: CPU HPA with minReplicas: 2. Later: migrate to lag‑based HPA.

⸻

5) Operational Playbooks

A) Create / Alter Topics

Admin won’t shrink partitions; to increase, alter existing topics.

# Describe (inside broker pod)
kubectl -n payment exec -it kafka-controller-0 -c kafka -- /bin/sh -lc \
'/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic payment_order_created_topic'

# Alter to 8 partitions (repeat for retry/succeeded and their .DLQ)
kubectl -n payment exec -it kafka-controller-0 -c kafka -- /bin/sh -lc \
"/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic payment_order_created_topic --partitions 8"

B) Verify Consumer Groups

# Inside broker pod
kubectl -n payment exec -it kafka-controller-0 -c kafka -- /bin/sh -lc \
'/opt/bitnami/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group payment-order-created-consumer-group'

Expect 8 partitions listed with owners when 2 pods × 4 listeners are running.

C) Deploy / Scale Consumers

# Deterministic baseline
# values.yaml: autoscaling.enabled=false, replicaCount=2
# Or CPU HPA with: minReplicas: 2, maxReplicas: 2

# Rollout restart
ykubectl -n payment rollout restart deployment/payment-consumers

D) Port‑forward from laptop (optional)

kubectl -n payment port-forward svc/kafka 9092:9092
# then add to /etc/hosts once: 127.0.0.1 kafka.payment.svc.cluster.local
kafka-consumer-groups --bootstrap-server kafka.payment.svc.cluster.local:9092 --list


⸻

6) Load & Validation Checklist
	1.	Start at 10 rps (≈ 20 ev/s created).
	2.	Watch group lag, queueing p95, commit p95.
	3.	Step to 30 rps, then 60 rps; ensure lag slope returns to ~0 within 1 min.
	4.	Verify retry topic load ≈ created_rate × r/(1−r).
	5.	Chaos: roll 1 consumer pod and the broker; confirm EOS (no dup processing) and fast catch‑up.

⸻

7) Troubleshooting

Symptoms → Checks → Fix
	•	Lag climbs steadily
	•	Check total listeners = partitions; pods × concurrency should equal 8 (or desired).
	•	Verify commit p95; if > target, increase total concurrency or investigate PSP latency.
	•	“No active members”
	•	Consumers not running / wrong bootstrap / wrong group id.
	•	Rebalance storms / CommitFailedException
	•	max.poll.interval.ms too low for batch; reduce max-poll-records or raise interval.
	•	Ensure CooperativeStickyAssignor is on consumer.
	•	DLQ publish succeeds but consumer keeps retrying the same record
	•	DLQ partitions must match source partition count.
	•	Throughput doesn’t change when increasing pools
	•	Listener blocks on PSP get(1s); pool threads ≥ listener concurrency per pod; big queues hide back‑pressure.
	•	Partitions increased but throughput didn’t
	•	Annotation concurrency="1" overrides factory; remove it.

⸻

8) Future Scaling Plan
	•	Toward 90 rps (~180 ev/s created):
	•	Likely 16–24 partitions and matching listeners.
	•	Consider non‑blocking PSP client with timeouts (remove pool/blocking).
	•	Outbox: switch to batched sends in one transaction (e.g., 100–200 msgs/tx) for higher producer throughput.
	•	Lag‑based HPA (Prometheus Adapter using kafka_consumergroup_lag).

⸻

9) Useful One‑Liners

# Topics (inside broker pod)
kubectl -n payment exec -it kafka-controller-0 -c kafka -- /bin/sh -lc \
'/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic payment_order_created_topic'

# Consumer groups (inside broker pod)
kubectl -n payment exec -it kafka-controller-0 -c kafka -- /bin/sh -lc \
'/opt/bitnami/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group payment-order-created-consumer-group'

# Watch lag from laptop (with port-forward)
watch -n 2 'kafka-consumer-groups --bootstrap-server kafka.payment.svc.cluster.local:9092 \
 --describe --group payment-order-created-consumer-group'


⸻

10) Appendix — Config Snippets

Listener factory (Kotlin)

containerProperties.ackMode = AckMode.MANUAL
isBatchListener = false
setConcurrency(cfg.concurrency) // from app.kafka.dynamic-consumers

@KafkaListener

@KafkaListener(
  topics = [Topics.PAYMENT_ORDER_CREATED],
  containerFactory = "${Topics.PAYMENT_ORDER_CREATED}-factory",
  groupId = CONSUMER_GROUPS.PAYMENT_ORDER_CREATED
  // no concurrency attribute here
)

PSP pool (per pod)

corePoolSize = listenerConcurrencyPerPod // e.g., 4
maxPoolSize = corePoolSize + 2
setQueueCapacity(0) // or small (≤16)
setRejectedExecutionHandler(CallerRunsPolicy())

Producer

spring.kafka.producer.properties:
  enable.idempotence: true
  linger.ms: 5
  compression.type: lz4

Consumer

spring.kafka.consumer:
  enable-auto-commit: false
  isolation.level: read_committed
  max-poll-records: 25
  properties:
    fetch.max.wait.ms: 50
    partition.assignment.strategy: org.apache.kafka.clients.consumer.CooperativeStickyAssignor


⸻

Notes
	•	Increasing partitions changes key→partition mapping; per‑key ordering isn’t preserved across an increase (acceptable here because created/retry/succeeded are separate flows keyed by order id).
	•	Always keep DLQ partition count equal to its source.
	•	Treat CPU HPA as baseline only; lag‑based HPA reflects the real bottleneck.

⸻

11) Why‑First Playbook (decision tree + rationale)

Goal: every action has a Why, a How, a Verify, and a Rollback.

Decision Tree — Rising Consumer Lag
	1.	Confirm it’s real
Why: Avoid chasing transient spikes.
How: Check group lag trend ≥ 2–3 polling intervals; verify fetch latency and commit p95.
	2.	Is total concurrency < partitions?
Why: Idle partitions cap throughput.
Action: Raise listener concurrency (pods × per‑pod concurrency) up to current partition count.
Verify: All partitions show an owner; lag slope flattens within 60s.
Rollback: Scale back concurrency.
	3.	Is arrival_rate × commit_p95 > total_concurrency?
Why: Queueing theory (Little’s Law / utilization).
Action A (fast): Increase total listeners (pods or per‑pod conc).
Action B (structural): Increase partitions (safe online).
When to pick B: If you’ll need sustained parallelism beyond current partitions; or CPU/HPA is already at min pods.
Caveat: Key→partition mapping changes when increasing partitions; per‑key order across the change isn’t guaranteed.
	4.	Service time (PSP p95) jumped?
Why: Longer per‑record time reduces capacity (μ).
Action:
	•	Tighten PSP timeout (e.g., 1s) and map timeout → TIMEOUT status (already done).
	•	Degrade scenario / enable circuit breaker to shed slow paths.
Verify: pspMs p95 and commit p95 fall; backlog clears.
Rollback: Restore normal scenario.
	5.	Backpressure producers if needed
Why: If λ (ingest) >> capacity and you cannot add partitions immediately, throttle to protect SLOs.
Action: Limit web RPS or outbox dispatcher threads temporarily.
Verify: Lag stabilizes; error rate does not rise.
	6.	If DLQ floods
Why: Poison messages / systemic bug.
Action: Pause affected listener container; inspect latest DLQ samples; roll a fix.
Verify: DLQ growth slows; main flow healthy.
Rollback: Resume container.

Action Cards (What / Why / How / Verify / Rollback)

Increase per‑pod listener concurrency
	•	Why: Raise parallelism until you saturate partitions.
	•	How: Set app.kafka.dynamic-consumers[*].concurrency (and remove any @KafkaListener(concurrency=...)).
	•	Verify: consumer-groups --describe shows all partitions owned; commit p95 ≤ target.
	•	Rollback: Restore previous concurrency and restart.

Increase partitions
	•	Why: Structural throughput increase beyond current cap.
	•	How: kafka-topics --alter --partitions <N> for source + .DLQ.
	•	Verify: PartitionCount: N; lag slope flattens after rebalance.
	•	Rollback: Not supported (Kafka cannot shrink partitions). Plan ahead.

Throttle ingress
	•	Why: Protect downstreams/SLOs while fixing capacity.
	•	How: API rate‑limit, or reduce outbox dispatcher threads.
	•	Verify: Lag stabilizes; 5xx does not spike.
	•	Rollback: Remove throttle gradually.

Switch PSP to degraded/cb
	•	Why: Cut slow paths to preserve throughput.
	•	How: Flip simulator to DEGRADED/enable timeouts/circuit breaker.
	•	Verify: pspMs p95 and commit p95 drop; lag clears.
	•	Rollback: Return to NORMAL.

⸻

12) “Why” behind key knobs
	•	Partitions: Multiply the max usable consumer threads; also isolate hotspots by key.  Why → Throughput scales ~linearly with partitions until producer/DB bound.
	•	Concurrency: Unlocks parallelism up to partition count.  Why → One consumer thread ≈ one in‑flight record (given per‑record tx).
	•	ackMode=MANUAL + EOS: Commit DB + offsets atomically.  Why → Prevents duplicates/holes during failures.
	•	max-poll-records=25 & max.poll.interval.ms: Keep work units bounded so the consumer re‑polls well before the interval.  Why → Avoid rebalances/commit failures.
	•	CooperativeStickyAssignor: Faster, minimal‑movement rebalances.  Why → Keeps more partitions processing during scale events.
	•	PSP timeouts/pool sizing: One pool thread per listener thread (blocking model).  Why → Extra threads don’t add throughput; they just hide back‑pressure.

⸻

13) Pre‑Change / Post‑Change Checklist (Ops template)

Before
	•	Announce window & owner in chat; link to this runbook.
	•	Snap current: topic describe, consumer-groups --describe, and key p95s.
	•	Confirm rollback steps (image tag / values.yaml).

During
	•	Apply one change at a time (e.g., raise partitions to 16).
	•	Watch: group lag, commit p95, rebalances, error rates.

After
	•	Capture “Verify” evidence (screens/CLI outputs).
	•	Write 3 bullets: What changed, Why now, How we know it worked.

⸻

14) GameDay Scenarios (practice the Why)
	1.	PSP spikes to 2s p95 → Expect queueing; apply degraded PSP/circuit breaker; measure recovery time.
	2.	Rebalance storm (roll both consumer pods) → Ensure lag clears < 2 min; verify no duplicates.
	3.	DLQ surge → Pause one listener; sample messages; patch & resume.

⸻

15) Communication Snippets
	•	“We’re seeing lag rising to X with commit p95 Y. Increasing total listeners from A→B (Why: capacity = λ×W). Verifying assignments and p95s; rollback is to restore concurrency.”
	•	“Partitions increased to N (Why: durable parallelism). Verified by PartitionCount: N; rebalanced; lag slope flattened in 60s.”

⸻

16) Lag‑based HPA (scale by Kafka consumer lag)

Scale pods using Kafka lag instead of CPU. We already run kafka-exporter, so we can map its metrics to Kubernetes External Metrics via Prometheus Adapter.

A) Install Prometheus Adapter

If not already installed:

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm upgrade --install prometheus-adapter prometheus-community/prometheus-adapter \
  -n monitoring \
  -f values-prom-adapter.yaml

values-prom-adapter.yaml (example)

rules:
  default: false
  custom:
    # Expose total lag per consumer group as an external metric
    - seriesQuery: 'kafka_consumergroup_lag{group!=""}'
      resources: {}
      name:
        as: "kafka_consumer_group_lag"
      metricsQuery: |
        sum(max_over_time(kafka_consumergroup_lag{<<.LabelMatchers>>}[1m])) by (group)

This publishes an external metric named kafka_consumer_group_lag labeled by group.

Verify it’s visible:

kubectl get --raw \
  "/apis/external.metrics.k8s.io/v1beta1/namespaces/payment/externalmetrics/kafka_consumer_group_lag" | jq

B) Create the HPA for payment-consumers

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: payment-consumers
  namespace: payment
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: payment-consumers
  minReplicas: 2
  maxReplicas: 6
  metrics:
  - type: External
    external:
      metric:
        name: kafka_consumer_group_lag
        selector:
          matchLabels:
            group: payment-order-created-consumer-group   # scale on the CREATED stream
      target:
        type: AverageValue
        averageValue: "500"   # target backlog per pod
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

How the math works: with AverageValue, HPA treats the metric as total lag ÷ replicas. Desired replicas ≈ ceil(totalLag / 500). Example: if lag = 4,000 → replicas ≈ 8.

Keep per‑pod listener concurrency fixed (e.g., 4). HPA changes pod count; total listeners = pods × concurrency.

C) Choosing the target (averageValue)

Pick a backlog budget per pod:

per_pod_throughput ≈ listener_concurrency_per_pod × (1 / commit_p95)
backlog_budget_seconds ≈ 30–60 s
averageValue ≈ per_pod_throughput × backlog_budget_seconds

Example (NORMAL): commit_p95 ≈ 0.25 s, concurrency=4 → per‑pod ≈ 16 msg/s. For a 30s backlog budget → ~480 ≈ 500 (used above).

D) Guardrails & notes
	•	Partitions cap throughput: if pods × concurrency > partitions, extra pods won’t help. Raise partitions first.
	•	Min replicas: keep minReplicas: 2 so a single pod failure doesn’t stall the group.
	•	Multiple groups: create a second HPA instance for the retry group if needed by changing the label selector.
	•	Cool‑down: scale‑down stabilization avoids flapping when lag clears.
	•	Observability: Grafana panel for the HPA metric + events (kubectl describe hpa).

E) Test plan
	1.	Set maxReplicas: 2 (deterministic) → run 10 rps and record lag/commit p95.
	2.	Raise to maxReplicas: 6, keep averageValue: 500.
	3.	Send a burst (e.g., 60 rps for 2–3 min). HPA should scale up; lag should settle to ≈ replicas × 500.
	4.	Stop the burst → verify gradual scale‑down after 5 min stabilization.