Journey of Scalability, Reliability & Observability Milestones

1. Baseline: Naive Synchronous Flow
   ‚Ä¢ Everything was handled in a single HTTP request/response.
   ‚Ä¢ Lesson: Blocking on I/O (e.g., external PSP calls) made the system fragile and slow under load. Quick bottleneck:
   HTTP thread pool saturated.

‚∏ª

2. Introduced Outbox Pattern & Asynchronous Processing
   ‚Ä¢ Moved PSP calls to Kafka-driven consumers.
   ‚Ä¢ HTTP endpoint now just enqueues the payment/order, responds immediately (202 Accepted).
   ‚Ä¢ Lesson: Decoupling request handling from downstream processing is critical for throughput and resilience.


3. Bottleneck #1: Downstream System/PSP Throughput
   ‚Ä¢ First real scalability limit: PSP calls (simulated or real) are much slower than Kafka queueing or DB.
   ‚Ä¢ Lesson: System throughput is always limited by the slowest downstream service.
   You observed that increasing HTTP capacity (threads/users) didn‚Äôt help if PSP remained slow.
   How did you solve thus bottleneck?

. Observable Symptoms
‚Ä¢ Kafka consumer lag started to grow, even though your HTTP requests (via k6) were being accepted quickly and Tomcat
threads were not overloaded.
‚Ä¢ HTTP requests returned 202/200 quickly, but ‚Äúdownstream processing‚Äù (payments actually being processed) was much
slower.
‚Ä¢ CPU, DB, Tomcat thread pool, and Kafka itself were not saturated. No other obvious resource spike

Your Log Sequence (in order):

1. PSP cache lookup:
   TIMING: PSP cache lookup took 0 ms for 43040
   üëâ Interpretation: No time wasted looking up cached results. Good.
2. PSP call (real, including cache):
   TIMING: Real PSP call took 2382 ms for paymentOrderId=43040
   TIMING: PSP call (including cache, if miss) took 2383 ms for 43040
   üëâ Interpretation:
   ‚Ä¢ Your network call to PSP took ~2.3 seconds.
   ‚Ä¢ This is the main contributor to total latency for this payment.
3. PSP returned:
   ‚úÖ PSP returned SUCCESSFUL for 43040
   üëâ Interpretation: Call succeeded.
4. processPspResult timings:
   ‚Ä¢ TIMING: processPspResult (DB/write) took 5 ms for 43040
   ‚Ä¢ TIMING: processPspResult (Total) took 4 ms for 43040
   üëâ Interpretation:
   ‚Ä¢ Your actual logic for handling and persisting the result is very fast (just 4‚Äì5 ms for DB write and total logic).
5. Handler total time:
   TIMING: Total handler time: 2388 ms for 43040
   üëâ Interpretation:
   ‚Ä¢ Your end-to-end handler (from message received until fully processed) is almost entirely dominated by the PSP call
   time.

‚∏ª

What‚Äôs the Bottleneck?

Not your DB. Not your app logic. Not your threading.
‚Ä¢ PSP call (external dependency) is your bottleneck.
‚Ä¢ DB writes, cache lookups, and processing are nearly instant.

‚∏ª

Visual Interpretation (from your logs):
![](/Users/dogancaglar/Desktop/Screenshot 2025-06-15 at 13.55.13.png)

Conclusion:

The slow part is outside your service, in the PSP (simulated) response time.
If your PSP gets faster, your whole system gets faster.

Step-by-Step Guide to Diagnose Resource Efficiency

1. Simulate a Fast PSP
   ‚Ä¢ Make the PSP return instantly (e.g., sleep = 0ms or just return immediately).
   ‚Ä¢ This way, your application logic and DB become the main factors.
   ‚Ä¢ The ‚ÄúTotal handler time‚Äù should drop dramatically (probably just a few ms per payment order).


2. Increase Load
   ‚Ä¢ Run a heavy load test (e.g., K6, Locust, JMeter, or your custom tool) with hundreds or thousands of concurrent
   requests.
   ‚Ä¢ Goal: See how many requests per second (RPS) your app can handle when it‚Äôs not blocked by external calls.

   ACTION
   ‚Ä¢ A. Load Test & Metrics
   ‚Ä¢ You ran a K6 load test with 600 VUs for 10 minutes.
   ‚Ä¢ Grafana/Prometheus dashboards showed:
   ‚Ä¢ DB Pool Utilization spiking
   ‚Ä¢ HTTP Request Rate dropping
   ‚Ä¢ Increased HTTP latency (p99, p95)
   ‚Ä¢ Outbox event backlog growing.
   ‚Ä¢ Kibana logs and infrastructure logs didn‚Äôt show app-level errors, confirming the bottleneck was not infra-related (
   CPU, memory, etc).
   ‚Ä¢ B. Database Query Analysis
   ‚Ä¢ You enabled pg_stat_statements to track slowest queries.
   ‚Ä¢ Ran a query on pg_stat_statements to list top time-consuming queries.
   ‚Ä¢ Found your biggest time sinks were:
   ‚Ä¢ SELECT ... FROM outbox_event WHERE id=?
   ‚Ä¢ SELECT ... FROM outbox_event WHERE status=? ORDER BY created_at LIMIT ?
   ‚Ä¢ INSERT INTO outbox_event ...
   ‚Ä¢ SELECT count(id) FROM outbox_event WHERE status=?
   ‚Ä¢ C. Application Code Mapping
   ‚Ä¢ You mapped these queries to specific repository methods:
   ‚Ä¢ Example: findByStatusOrderByCreatedAtAsc(status: String, pageable: Pageable)
   (Maps to: SELECT ... WHERE status=? ORDER BY created_at LIMIT ?)
   ‚Ä¢ Noted that this was being called very frequently by your outbox dispatcher scheduler.

Step-by-Step: Database Improvements and Configuration Changes

1. Enabling pg_stat_statements

Goal:
Get visibility into which queries are slow or called most often.

Actions:
‚Ä¢ Tried CREATE EXTENSION pg_stat_statements;
‚Üí Got error: ‚Äúmust be loaded via shared_preload_libraries‚Äù
‚Ä¢ Changed DB configuration:
‚Ä¢ Edited postgresql.conf
‚Ä¢ Added/modified:
SELECT
query,
calls,
total_exec_time AS total_time,
mean_exec_time AS mean_time,
rows,
(total_exec_time / calls) AS avg_time_per_call
FROM
pg_stat_statements
ORDER BY
total_exec_time DESC
LIMIT 10;

How You Identified the Problem (Your Process):

1. Monitor: Noticed high DB times and slow endpoints via Prometheus/Grafana.
2. Investigate: Used pg_stat_statements to find slowest/top total time queries.
3. Analyze: Ran EXPLAIN ANALYZE on those queries.
4. Fix: Found missing index ‚Üí created it.
5. Verify: Saw improved metrics and faster response time.

no pk on payment table and no index on status column of outbox table.

4. Watch Your Metrics
   ‚Ä¢ CPU usage: Is it spiking? Are you fully utilizing your CPUs or sitting idle?
   ‚Ä¢ Thread pools: Are threads waiting or maxed out? (Spring Boot actuator endpoints and JVM tools like VisualVM can
   show this.)
   ‚Ä¢ Database:
   ‚Ä¢ Look at DB CPU usage, connection pool stats, and query latencies.
   ‚Ä¢ If your DB is the bottleneck, you‚Äôll see handler times increase as you push more load.
   ‚Ä¢ Queue sizes (if you use async queues): Are any internal queues (Kafka, Redis, etc.) backing up?

4. Monitor Handler Time Distribution
   ‚Ä¢ With PSP ‚Äúout of the way,‚Äù any increase in total handler time is now due to your code or your DB.
   ‚Ä¢ Use your existing Kibana visualizations for average/max/percentile handler times.

5. Watch for Saturation or Errors
   ‚Ä¢ Look for signs of saturation:
   ‚Ä¢ Growing HTTP response times
   ‚Ä¢ Increased error rates (timeouts, 5xx)
   ‚Ä¢ Thread pool exhaustion (can‚Äôt accept new tasks)
   ‚Ä¢ If you see this before hardware is maxed, you may have hidden bottlenecks (e.g., locking, connection pool limits,
   non-optimal queries).

‚∏ª

What Will This Show?
‚Ä¢ If your app is efficient:
‚Ä¢ You should be able to process hundreds/thousands of payment orders per second per core/thread, limited by your
database or hardware.
‚Ä¢ CPU should be busy but not pegged at 100%.
‚Ä¢ No weird spikes in handler times or thread pool rejections.
‚Ä¢ If you have an inefficiency:
‚Ä¢ You‚Äôll see handler times increase as load increases, even though PSP is fast.
‚Ä¢ You might see high CPU usage with little throughput (bad code, contention, blocking).
‚Ä¢ Thread pool or DB connection pool maxed out.
‚Ä¢ Garbage collection or locking spikes (rare, but possible)..

        Metric Correlation

‚Ä¢ You monitored:
‚Ä¢ PSP call latency metric (via Micrometer timer in safePspCall)
‚Ä¢ Consumer processing time
‚Ä¢ Both metrics showed that each payment event spent most of its consumer time waiting for the PSP simulation (often
100ms‚Äì2000ms+ depending on the scenario).

      Scaling Experiment

‚Ä¢ You tried increasing HTTP concurrency (more k6 users, more Tomcat threads).
‚Ä¢ Result: No improvement in the downstream processing speed‚Äîconsumer lag continued to increase if load > PSP capacity.
‚Ä¢ You tried scaling Kafka consumers (more consumer threads/partitions).
‚Ä¢ Result: No improvement if PSP calls remained slow; improvement only if you had more partitions AND the PSP was not
saturated

    Direct Observation

‚Ä¢ PSP scenario simulation: When you changed the psp.simulation.currentScenario to slower/faster settings, the downstream
processing directly sped up or slowed down.
‚Ä¢ Logs: Your consumer logs showed that safePspCall was always the slowest part (calls were taking a long time and often
timing out).

5. Process of Elimination
   ‚Ä¢ DB, HTTP server, Kafka, and network all showed low or moderate usage.
   ‚Ä¢ Only the PSP call duration was consistently high.
   ‚∏ª

4. Introduced Observability: Prometheus & Grafana
   ‚Ä¢ Tracked key metrics:
   ‚Ä¢ HTTP request/sec, latency distributions
   ‚Ä¢ Kafka consumer lag
   ‚Ä¢ PSP call duration
   ‚Ä¢ Tomcat thread pool utilization
   ‚Ä¢ Database connection pool saturation
   ‚Ä¢ Lesson: You can‚Äôt optimize what you don‚Äôt measure. The right metrics make bottlenecks obvious.

‚∏ª

5. Bottleneck #2: Tomcat Thread Pool
   ‚Ä¢ Observed: Under load, Tomcat max threads were exhausted (200 busy of 200 max).
   ‚Ä¢ Lesson: Even with async, you need enough HTTP threads to handle incoming traffic‚Äîbut just increasing them only
   helps until the next bottleneck.

‚∏ª

6. Monitoring Kafka: Lag and Throughput
   ‚Ä¢ Tracked:
   ‚Ä¢ kafka_consumer_fetch_manager_records_lag_max
   ‚Ä¢ Observed consumer lag growing when downstream was slow.
   ‚Ä¢ Lesson: Kafka lag is the ‚Äúcanary‚Äù of your pipeline. If lag spikes, your consumers can‚Äôt keep up.

‚∏ª

7. Scaling Out Consumers
   ‚Ä¢ Lesson: For partitioned Kafka topics, you can scale horizontally (add more consumers/pods) to process more in
   parallel‚Äîup to the number of partitions.

‚∏ª

8. Partitioning Kafka Topics by aggregateId
   ‚Ä¢ Introduced: Partitioning by aggregateId (e.g., paymentOrderId) to guarantee in-order processing for a single
   aggregate.
   ‚Ä¢ Lesson: Partitioning strategy is fundamental for both scalability and correctness.
   ‚Ä¢ Partitioning enables parallelism and enforces order for the same key.

‚∏ª

9. Reliability: Defensive Event Processing
   ‚Ä¢ Lesson:
   ‚Ä¢ Don‚Äôt trust producers.
   ‚Ä¢ Always check, ‚Äúdoes the referenced aggregate exist?‚Äù before processing events.
   ‚Ä¢ With partitioning, you can also buffer/retry events that arrive out of order.
   ‚Ä¢ Without partitioning, this is nearly impossible.

‚∏ª

10. Performance Monitoring & Alerts
    ‚Ä¢ Set up:
    ‚Ä¢ Alerting on high HTTP/PSP latency, thread pool saturation, Kafka lag, and DB pool usage.
    ‚Ä¢ Lesson: Early warnings (alerts) help you react before the system falls over.

‚∏ª

11. Scaling Limits & Systemic Trade-offs
    ‚Ä¢ Lesson:
    ‚Ä¢ You can only scale each component up to its own bottleneck.
    ‚Ä¢ You must know which part (web server, consumer, PSP, DB, Kafka) is the current bottleneck, and design accordingly.
    ‚Ä¢ Elastic scaling (horizontal) is easier with stateless, partitioned, async components.

‚∏ª

12. Reliability Features
    ‚Ä¢ Implemented:
    ‚Ä¢ Idempotency (to prevent double-processing)
    ‚Ä¢ Retry & DLQ (for transient and permanent failures)
    ‚Ä¢ Observability (logging with trace IDs)
    ‚Ä¢ Lesson: High throughput systems must be both scalable and resilient‚Äîable to recover from, and surface, errors.

‚∏ª

Summary of Key Takeaways
‚Ä¢ Never trust the producer: Always defensively check data.
‚Ä¢ Measure everything: Bottlenecks are only obvious with the right metrics and dashboards.
‚Ä¢ Partitioning is not just for scaling: It guarantees order and enables reliable event-driven orchestration.
‚Ä¢ Async architecture improves both throughput and resilience‚Äîbut only if you monitor, alert, and tune every hop.
‚Ä¢ You hit limits one by one: PSP, HTTP, thread pools, consumers, DB pool‚Ä¶ so you have to continuously tune and
re-architect for the next weakest link.

‚∏ª

Let me know if you want this formatted differently (e.g., table, bullet-only), or want any code/diagram illustrations
for your eventual write-up or presentation!