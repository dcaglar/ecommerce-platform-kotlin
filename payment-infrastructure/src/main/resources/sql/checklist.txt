Below is a pragmatic, step-by-step tuning sequence for the exact setup you posted.
I’m ordering the levers from lowest risk / fastest feedback to structural refactors; work down the list until the pool-exhaustion symptoms disappear or relocate.

⸻

1 – Give the HTTP request path a little more head-room first


What
How
Why
Bump maximum-pool-size from 70 → 90 – 110
yaml<br>hikari:<br>  maximum-pool-size: 100  # start here, adjust ±10<br>  minimum-idle: 20       # keep<br>  connection-timeout: 1000  # see next row<br>
70 active conns + 8 outbox workers + some Kafka admin clients can saturate the pool. Another ~30 – 40 conns usually buys ~50 % more throughput without swamping the DB CPU.
Tighten connection-timeout from 3000 ms → 1000 ms
same block
Right now a thread can wait 3 s for a conn → that’s the spike you saw in “DB Acquire Time”. Failing fast (1 s) returns 503 quickly and protects Tomcat’s queue.



2 – Isolate or throttle the Outbox writers

The dispatcher fires 8*250 = 2 000 rows every 3 s on the same pool the API uses.

Option A – Separate pool (cleanest)

@Bean("outboxDataSource")
fun outboxDataSource(main: DataSource): DataSource =
    DataSourceBuilder.create()
        .type(HikariDataSource::class.java)
        .driverClassName(main.driverClassName)
        .url(main.jdbcUrl)
        .username(main.username)
        .password(main.password)
        .build().apply {
            (this as HikariDataSource).maximumPoolSize = 16   // isolated cap
        }

